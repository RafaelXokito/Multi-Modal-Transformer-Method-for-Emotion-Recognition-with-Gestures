{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import optuna\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import atexit\n",
    "from IPython import get_ipython\n",
    "\n",
    "# Get the IPython kernel instance\n",
    "kernel = get_ipython().kernel\n",
    "\n",
    "# Register a function to be called when the script exits\n",
    "@atexit.register\n",
    "def shutdown_kernel():\n",
    "    kernel.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoader(path):\n",
    "    return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"vgg\"\n",
    "# Number of epochs to train for \n",
    "num_epochs = 30\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "def load_data(batch_size=32): # Batch size for training (change depending on how much memory you have)\n",
    "\n",
    "    # Applying Transforms to the Data\n",
    "    image_transforms = { \n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((228, 228)),\n",
    "            transforms.RandomCrop(size=224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.0024, 0.0016, 0.0014, 0.0002],\n",
    "                                [0.0403, 0.0293, 0.0260, 0.0035])\n",
    "        ]),\n",
    "        'valid': transforms.Compose([\n",
    "            transforms.Resize((228, 228)),\n",
    "            transforms.RandomCrop(size=224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.0024, 0.0016, 0.0014, 0.0002],\n",
    "                                [0.0403, 0.0293, 0.0260, 0.0035])\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            # transforms.RandomCrop(size=224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.0024, 0.0016, 0.0014, 0.0002],\n",
    "                                [0.0403, 0.0293, 0.0260, 0.0035])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Load the Data\n",
    "    # Set train and valid directory paths\n",
    "\n",
    "    dataset_directory = r\"../datasets/AffectNet_10Percent_SegmentedFaces_RGBA\"\n",
    "\n",
    "    train_directory = os.path.join(dataset_directory, 'train')\n",
    "    valid_directory = os.path.join(dataset_directory, 'valid')\n",
    "    test_directory = os.path.join(dataset_directory, 'test')\n",
    "\n",
    "    # Number of classes\n",
    "    num_classes = len(os.listdir(valid_directory))\n",
    "        \n",
    "    # Load Data from folders\n",
    "    data = {\n",
    "        'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train'], loader=lambda path: customLoader(path)),\n",
    "        'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'], loader=lambda path: customLoader(path)),\n",
    "        'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'], loader=lambda path: customLoader(path))\n",
    "    }\n",
    "\n",
    "    # Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "    idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "    print(idx_to_class)\n",
    "\n",
    "    # Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "    train_data_size = len(data['train'])\n",
    "    valid_data_size = len(data['valid'])\n",
    "    test_data_size = len(data['test'])\n",
    "\n",
    "    # Create iterators for the Data loaded using DataLoader module\n",
    "    train_data_loader = DataLoader(data['train'], batch_size=batch_size, shuffle=True)\n",
    "    valid_data_loader = DataLoader(data['valid'], batch_size=batch_size, shuffle=True)\n",
    "    test_data_loader = DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return num_classes, idx_to_class, train_data_size, valid_data_size, test_data_size, train_data_loader, valid_data_loader, test_data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, criterion, optimizer, train_data_loader, valid_data_loader, device, train_data_size, valid_data_size, history_path, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_loss = 100000.0\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss - by loss function \"criterion\"\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the gradients - altera os pesos\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters - aplicação de alterações\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions)) # - contar quantos acertou\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "        \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        log = \"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start)\n",
    "        print(log)\n",
    "        with open(os.path.join(history_path, 'train.log'), 'a') as the_file:\n",
    "            the_file.write(log + '\\n')\n",
    "\n",
    "        # Save if the model has best accuracy till now\n",
    "        if epoch == num_epochs -1 :\n",
    "            torch.save(model, os.path.join(history_path, 'last_model.pt'))\n",
    "            model.load_state_dict(best_model)\n",
    "            torch.save(model, os.path.join(history_path, f'best_model_epoch{best_epoch+1}.pt'))\n",
    "            \n",
    "    return model, history, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch model using a pre-trained ResNet18 model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        self.features = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.features.fc = nn.Sequential(\n",
    "            nn.Linear(self.features.fc.in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, num_layers=None, hidden_size=None, dropout=None):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet_\":\n",
    "        \"\"\" Resnet\n",
    "        \"\"\"\n",
    "        model_ft = MyModel(num_layers=num_layers, hidden_size=hidden_size, dropout=dropout)\n",
    "\n",
    "        # Modify the first convolutional layer to have 4 input channels instead of 3\n",
    "        model_ft.conv1 = torch.nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        set_parameter_requires_grad(model_ft)\n",
    "        model_ft.conv1.requires_grad = True\n",
    "\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "        # Modify the first convolutional layer to have 4 input channels instead of 3\n",
    "        model_ft.conv1 = torch.nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        set_parameter_requires_grad(model_ft)\n",
    "        model_ft.conv1.requires_grad = True\n",
    "\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        #model_ft = torch.load(\"model_999_t3.pt\")\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)       \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept input with 4 channels\n",
    "        new_layer = torch.nn.Conv2d(4, 64, kernel_size=3, padding=1)\n",
    "        old_weights = model_ft.features[0].weight.data\n",
    "        new_layer.weight.data[:, :3, :, :] = old_weights\n",
    "        new_layer.weight.data[:, 3, :, :] = old_weights[:, 0, :, :]\n",
    "        model_ft.features[0] = new_layer\n",
    "\n",
    "        set_parameter_requires_gradTrue_nofc(model_ft)\n",
    "        \n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        print(num_classes)\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(weights=None)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(weights=None)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze\n",
    "def set_parameter_requires_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "#UnFreeze\n",
    "def set_parameter_requires_gradTrue(model):\n",
    "    num=0\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        # if num >= 30 :\n",
    "        #     param.requires_grad = True\n",
    "        # num=num+1\n",
    "\n",
    "def set_parameter_requires_gradTrue_nofc(model):\n",
    "    num=0\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTestSetAccuracy(loss_criterion, num_classes, idx_to_class, test_data_size, test_data_loader, best_model, history_path):\n",
    "    '''\n",
    "    Function to compute the accuracy on the test set\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "    '''\n",
    "    # model = torch.load(os.path.join(history_path, \"model.pt\"))\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model = best_model\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
    "            inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # labels.cpu().numpy()\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Confusion matrix\n",
    "            for t, p in zip(labels.view(-1), predictions.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            log = \"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item())\n",
    "            print(log)\n",
    "            \n",
    "            with open(os.path.join(history_path, 'test.log'), 'a') as the_file:\n",
    "                the_file.write(log + '\\n')\n",
    "\n",
    "    # Find average test loss and test accuracy\n",
    "    avg_test_loss = test_loss/test_data_size \n",
    "    avg_test_acc = test_acc/test_data_size\n",
    "\n",
    "    cm = confusion_matrix.cpu().numpy()\n",
    "    plt.imshow(cm, cmap='gray_r')\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:.0f}\".format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > (cm.max() / 1.5) else \"black\")\n",
    "\n",
    "    tick_marks = np.arange(len(confusion_matrix))\n",
    "    plt.xticks(tick_marks, idx_to_class.values(), rotation=45)\n",
    "    plt.yticks(tick_marks, idx_to_class.values())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(history_path, 'confusion_matrix.png'), bbox_inches=\"tight\")\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    indiv_acc = (confusion_matrix.diag()/confusion_matrix.sum(1)).numpy()\n",
    "\n",
    "    print(\"Test Accuracy : \" + str(avg_test_acc))\n",
    "\n",
    "    print(\"Test Accuracy Per Class :\")\n",
    "\n",
    "    for key in idx_to_class:\n",
    "        print(f\"{idx_to_class[key]} - {round(indiv_acc[key] * 100, 4)}%\")\n",
    "\n",
    "    with open(os.path.join(history_path, 'test.log'), 'a') as the_file:\n",
    "        the_file.write(\"Test Accuracy : \" + str(avg_test_acc) + \"\\n\\n\")\n",
    "        the_file.write(\"Test Accuracy Per Class : \\n\")\n",
    "        for key in idx_to_class:\n",
    "            the_file.write(f\"{idx_to_class[key]} - {round(indiv_acc[key] * 100, 4)}%\\n\")\n",
    "\n",
    "    return avg_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to tune\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.0, 1.0)\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [optim.SGD, optim.Adam]),\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    num_classes, idx_to_class, train_data_size, valid_data_size, test_data_size, train_data_loader, valid_data_loader, test_data_loader = load_data(batch_size=batch_size)\n",
    "\n",
    "    # Initialize the model for this run\n",
    "    model_ft, input_size = initialize_model(model_name, num_classes, feature_extract)\n",
    "\n",
    "    set_parameter_requires_gradTrue(model_ft)\n",
    "            \n",
    "    # Device used to train the network (Ex: GPU)\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "\n",
    "    # Send the model to the processing unit\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # Gather the parameters to be optimized/updated in this run. If we are\n",
    "    #  finetuning we will be updating all parameters. However, if we are \n",
    "    #  doing feature extract method, we will only update the parameters\n",
    "    #  that we have just initialized, i.e. the parameters with requires_grad\n",
    "    #  is True.\n",
    "    params_to_update = model_ft.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    else:\n",
    "        for name,param in model_ft.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\", name)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    try:\n",
    "        optimizer_ft = optimizer[0](params_to_update, lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    except:\n",
    "        optimizer_ft = optimizer[0](params_to_update, lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "\n",
    "    curr_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    history_path = os.path.join('./history/', curr_time)\n",
    "\n",
    "    os.mkdir(history_path)\n",
    "\n",
    "    # Setup the loss fxn\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    best_model, history, best_epoch = train_and_validate(model_ft, criterion, optimizer_ft, train_data_loader, valid_data_loader, device, train_data_size, valid_data_size, history_path, epochs=num_epochs)\n",
    "\n",
    "    history = np.array(history)\n",
    "    plt.plot(history[:,0:2])\n",
    "    plt.legend(['Tr Loss', 'Val Loss'])\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.ylim(0,3)\n",
    "    plt.savefig(os.path.join(history_path, 'loss_curve.png'))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history[:,2:4])\n",
    "    plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0,1)\n",
    "    plt.savefig(os.path.join(history_path, 'accuracy_curve.png'))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    computeTestSetAccuracy(criterion, num_classes, idx_to_class, test_data_size, test_data_loader, best_model, history_path)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 23:57:12,330]\u001b[0m A new study created in memory with name: no-name-bcacbedc-136d-4249-bae4-558b95aa51b1\u001b[0m\n",
      "/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/distributions.py:535: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/distributions.py:535: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
      "  warnings.warn(message)\n",
      "/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'}\n",
      "8\n",
      "mps\n",
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.1.weight\n",
      "\t features.1.bias\n",
      "\t features.4.weight\n",
      "\t features.4.bias\n",
      "\t features.5.weight\n",
      "\t features.5.bias\n",
      "\t features.8.weight\n",
      "\t features.8.bias\n",
      "\t features.9.weight\n",
      "\t features.9.bias\n",
      "\t features.11.weight\n",
      "\t features.11.bias\n",
      "\t features.12.weight\n",
      "\t features.12.bias\n",
      "\t features.15.weight\n",
      "\t features.15.bias\n",
      "\t features.16.weight\n",
      "\t features.16.bias\n",
      "\t features.18.weight\n",
      "\t features.18.bias\n",
      "\t features.19.weight\n",
      "\t features.19.bias\n",
      "\t features.22.weight\n",
      "\t features.22.bias\n",
      "\t features.23.weight\n",
      "\t features.23.bias\n",
      "\t features.25.weight\n",
      "\t features.25.bias\n",
      "\t features.26.weight\n",
      "\t features.26.bias\n",
      "\t classifier.0.weight\n",
      "\t classifier.0.bias\n",
      "\t classifier.3.weight\n",
      "\t classifier.3.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch: 1/30\n",
      "Epoch : 000, Training: Loss - 2.0231, Accuracy - 20.5917%, \n",
      "\t\tValidation : Loss - 1.9282, Accuracy - 23.2441%, Time: 65.7800s\n",
      "Epoch: 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-02-16 23:58:24,308]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 0.0072566755084602745, 'momentum': 0.4936213959947444, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'weight_decay': 0.00014140684334574652, 'batch_size': 64} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rafaelpereira/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/rq/_h5pztyd1s1fvzt92rddhqhh0000gn/T/ipykernel_54131/1495904139.py\", line 58, in train_model\n",
      "    best_model, history, best_epoch = train_and_validate(model_ft, criterion, optimizer_ft, train_data_loader, valid_data_loader, device, train_data_size, valid_data_size, history_path, epochs=num_epochs)\n",
      "  File \"/var/folders/rq/_h5pztyd1s1fvzt92rddhqhh0000gn/T/ipykernel_54131/753123199.py\", line 55, in train_and_validate\n",
      "    train_loss += loss.item() * inputs.size(0)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-02-16 23:58:24,335]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m study\u001b[39m.\u001b[39;49moptimize(train_model, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m best_params \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m      4\u001b[0m best_accuracy \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_value\n",
      "File \u001b[0;32m~/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniforge3/envs/MMTMERG/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     55\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     57\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m best_model, history, best_epoch \u001b[39m=\u001b[39m train_and_validate(model_ft, criterion, optimizer_ft, train_data_loader, valid_data_loader, device, train_data_size, valid_data_size, history_path, epochs\u001b[39m=\u001b[39;49mnum_epochs)\n\u001b[1;32m     60\u001b[0m history \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(history)\n\u001b[1;32m     61\u001b[0m plt\u001b[39m.\u001b[39mplot(history[:,\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, criterion, optimizer, train_data_loader, valid_data_loader, device, train_data_size, valid_data_size, history_path, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[39m# Compute the total loss for the batch and add it to train_loss\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Compute the accuracy\u001b[39;00m\n\u001b[1;32m     58\u001b[0m ret, predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(train_model, n_trials=100)\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "print(\"Best parameters: \", best_params)\n",
    "print(\"Best accuracy: \", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from skopt import gp_minimize\n",
    "# from skopt.space import Real, Categorical, Integer\n",
    "# from torchvision import models\n",
    "\n",
    "# # Define a PyTorch model using a pre-trained ResNet18 model\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, num_layers, hidden_size, dropout):\n",
    "#         super().__init__()\n",
    "#         self.features = models.resnet18(pretrained=True)\n",
    "#         self.features.fc = nn.Sequential(\n",
    "#             nn.Linear(self.features.fc.in_features, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(p=dropout),\n",
    "#             nn.Linear(hidden_size, output_size)\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         return x\n",
    "\n",
    "# # Define hyperparameter search space\n",
    "# space = [\n",
    "#     Integer(16, 256, name='hidden_size'),\n",
    "#     Real(0.1, 0.5, name='dropout'),\n",
    "#     Categorical([optim.SGD, optim.Adam], name='optimizer'),\n",
    "#     Real(1e-5, 1e-1, prior='log-uniform', name='lr'),\n",
    "# ]\n",
    "\n",
    "# # Define cross-validation strategy\n",
    "# def train_val_split(X, y):\n",
    "#     # Define train/validation split\n",
    "#     train_idx = int(len(X) * 0.8)\n",
    "#     X_train, y_train = X[:train_idx], y[:train_idx]\n",
    "#     X_val, y_val = X[train_idx:], y[train_idx:]\n",
    "#     return X_train, y_train, X_val, y_val\n",
    "\n",
    "# # Define training loop\n",
    "# def train(model, X_train, y_train, X_val, y_val, optimizer, criterion, num_epochs=100, early_stopping_patience=10):\n",
    "#     best_loss = float('inf')\n",
    "#     early_stopping_counter = 0\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(X_train)\n",
    "#         loss = criterion(y_pred, y_train)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             y_pred_val = model(X_val)\n",
    "#             val_loss = criterion(y_pred_val, y_val)\n",
    "        \n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             early_stopping_counter = 0\n",
    "#         else:\n",
    "#             early_stopping_counter += 1\n",
    "            \n",
    "#         if early_stopping_counter >= early_stopping_patience:\n",
    "#             break\n",
    "            \n",
    "#     return best_loss.item()\n",
    "\n",
    "# # Define objective function for optimization\n",
    "# def objective(params):\n",
    "#     # Create model\n",
    "#     model = MyModel(num_layers=params[0], hidden_size=params[1], dropout=params[2])\n",
    "    \n",
    "#     # Freeze all layers except the last one\n",
    "#     for param in model.features.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     for param in model.features.fc.parameters():\n",
    "#         param.requires_grad = True\n",
    "    \n",
    "#     # Create optimizer\n",
    "#     optimizer = params[3](model.parameters(), lr=params[4])\n",
    "    \n",
    "#     # Train model\n",
    "#     criterion = nn.MSELoss()\n",
    "#     X_train, y_train, X_val, y_val = train_val_split(X, y)\n",
    "#     loss = train(model, X_train, y_train, X_val, y_val, optimizer, criterion)\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# # Load data\n",
    "# X, y = load_data()\n",
    "\n",
    "# # Perform hyperparameter search\n",
    "# result = gp_minimize(objective, space, n_calls=20)\n",
    "\n",
    "# # Print best hyperparameters and loss\n",
    "# print('Best hyperparameters:', result.x)\n",
    "# print('Best loss:', result.fun)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTMERG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d3c456ffaa9cd28f83f00357dff394a6c471d38d2284dc7337cbaff2c725068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
