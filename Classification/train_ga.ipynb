{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customLoader(path):\n",
    "    return Image.open(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'anger', 1: 'contempt', 2: 'disgust', 3: 'fear', 4: 'happy', 5: 'neutral', 6: 'sad', 7: 'surprise'}\n"
     ]
    }
   ],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms \n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "bs = 32\n",
    "# Number of epochs to train for \n",
    "num_epochs = 30\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "# Applying Transforms to the Data\n",
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((228, 228)),\n",
    "        transforms.RandomCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.0018, 0.0002],\n",
    "                             [0.0321, 0.0035])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize((228, 228)),\n",
    "        transforms.RandomCrop(size=224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.0018, 0.0002],\n",
    "                             [0.0321, 0.0035])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        # transforms.RandomCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.0018, 0.0002],\n",
    "                             [0.0321, 0.0035])\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Load the Data\n",
    "# Set train and valid directory paths\n",
    "\n",
    "dataset_directory = r\"../datasets/AffectNet_10Percent_SegmentedFaces_GA\"\n",
    "\n",
    "train_directory = os.path.join(dataset_directory, 'train')\n",
    "valid_directory = os.path.join(dataset_directory, 'valid')\n",
    "test_directory = os.path.join(dataset_directory, 'test')\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(os.listdir(valid_directory))\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train'], loader=lambda path: customLoader(path)),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid'], loader=lambda path: customLoader(path)),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'], loader=lambda path: customLoader(path))\n",
    "}\n",
    "\n",
    "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
    "idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
    "print(idx_to_class)\n",
    "\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train_data_loader = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid_data_loader = DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test_data_loader = DataLoader(data['test'], batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_validate(model, criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_loss = 100000.0\n",
    "    best_epoch = None\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "            inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute loss - by loss function \"criterion\"\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the gradients - altera os pesos\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters - aplicação de alterações\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions)) # - contar quantos acertou\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "        \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        log = \"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start)\n",
    "        print(log)\n",
    "        with open(os.path.join(history_path, 'train.log'), 'a') as the_file:\n",
    "            the_file.write(log + '\\n')\n",
    "\n",
    "        # Save if the model has best accuracy till now\n",
    "        if epoch == num_epochs -1 :\n",
    "            torch.save(model, os.path.join(history_path, 'last_model.pt'))\n",
    "            model.load_state_dict(best_model)\n",
    "            torch.save(model, os.path.join(history_path, f'best_model_epoch{best_epoch+1}.pt'))\n",
    "            \n",
    "    return model, history, best_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 2 input channels\n",
    "        old_weights = model_ft.conv1.weight\n",
    "        new_weights = torch.zeros((64, 2, 7, 7))\n",
    "        with torch.no_grad():\n",
    "            new_weights[:, :2, :, :] = old_weights[:, :2, :, :]\n",
    "        new_conv = torch.nn.Conv2d(2, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        with torch.no_grad():\n",
    "            new_conv.weight = torch.nn.Parameter(new_weights)\n",
    "\n",
    "        # Replace the first convolutional layer in the ResNet18 model with the new layer\n",
    "        model_ft.conv1 = new_conv\n",
    "\n",
    "        set_parameter_requires_grad(model_ft)\n",
    "\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        #model_ft = torch.load(\"model_999_t3.pt\")\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)       \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept 2 input channels\n",
    "        new_conv = torch.nn.Conv2d(2, 64, kernel_size=3, padding=1)\n",
    "        old_weights = model_ft.features[0].weight.data\n",
    "        new_weights = torch.FloatTensor(64, 2, 3, 3)\n",
    "        new_weights[:, 0:1, :, :] = old_weights[:, 0:1, :, :]\n",
    "        new_weights[:, 1:2, :, :] = old_weights[:, 0:1, :, :]\n",
    "        new_conv.weight.data = new_weights\n",
    "\n",
    "        # Replace the first convolutional layer in the VGG model with the new layer\n",
    "        model_ft.features[0] = new_conv\n",
    "\n",
    "        set_parameter_requires_gradTrue_nofc(model_ft)\n",
    "        \n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        print(num_classes)\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(weights=None)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(weights=None)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Freeze\n",
    "def set_parameter_requires_grad(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "            \n",
    "#UnFreeze\n",
    "def set_parameter_requires_gradTrue(model):\n",
    "    num=0\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        # if num >= 30 :\n",
    "        #     param.requires_grad = True\n",
    "        # num=num+1\n",
    "\n",
    "def set_parameter_requires_gradTrue_nofc(model):\n",
    "    num=0\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n",
      "Epoch: 1/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     42\u001b[0m \u001b[39m# Train and evaluate\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m best_model, history, best_epoch \u001b[39m=\u001b[39m train_and_validate(model_ft, criterion, optimizer_ft, epochs\u001b[39m=\u001b[39;49mnum_epochs)\n\u001b[1;32m     45\u001b[0m history \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(history)\n\u001b[1;32m     46\u001b[0m plt\u001b[39m.\u001b[39mplot(history[:,\u001b[39m0\u001b[39m:\u001b[39m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[119], line 55\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     52\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[39m# Compute the total loss for the batch and add it to train_loss\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem() \u001b[39m*\u001b[39m inputs\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39m# Compute the accuracy\u001b[39;00m\n\u001b[1;32m     58\u001b[0m ret, predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39mdata, \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract)\n",
    "\n",
    "set_parameter_requires_gradTrue(model_ft)\n",
    "         \n",
    "# Device used to train the network (Ex: GPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Send the model to the processing unit\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\", name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\", name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "curr_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "history_path = os.path.join('./history/', curr_time)\n",
    "\n",
    "os.mkdir(history_path)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "best_model, history, best_epoch = train_and_validate(model_ft, criterion, optimizer_ft, epochs=num_epochs)\n",
    "\n",
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,3)\n",
    "plt.savefig(os.path.join(history_path, 'loss_curve.png'))\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.savefig(os.path.join(history_path, 'accuracy_curve.png'))\n",
    "plt.show()\n",
    "\n",
    "def computeTestSetAccuracy(loss_criterion):\n",
    "    '''\n",
    "    Function to compute the accuracy on the test set\n",
    "    Parameters\n",
    "        :param model: Model to test\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "    '''\n",
    "    # model = torch.load(os.path.join(history_path, \"model.pt\"))\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    # Validation - No gradient tracking needed\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model = best_model\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation loop\n",
    "        for j, (inputs, labels) in enumerate(test_data_loader):\n",
    "            inputs = inputs.type(torch.FloatTensor).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # labels.cpu().numpy()\n",
    "\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            # Compute the total loss for the batch and add it to valid_loss\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "            # Confusion matrix\n",
    "            for t, p in zip(labels.view(-1), predictions.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "            # Compute total accuracy in the whole batch and add to valid_acc\n",
    "            test_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "            log = \"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item())\n",
    "            print(log)\n",
    "            \n",
    "            with open(os.path.join(history_path, 'test.log'), 'a') as the_file:\n",
    "                the_file.write(log + '\\n')\n",
    "\n",
    "    # Find average test loss and test accuracy\n",
    "    avg_test_loss = test_loss/test_data_size \n",
    "    avg_test_acc = test_acc/test_data_size\n",
    "\n",
    "    cm = confusion_matrix.cpu().numpy()\n",
    "    plt.imshow(cm, cmap='gray_r')\n",
    "    plt.colorbar()\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, \"{:.0f}\".format(cm[i, j]), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > (cm.max() / 1.5) else \"black\")\n",
    "\n",
    "    tick_marks = np.arange(len(confusion_matrix))\n",
    "    plt.xticks(tick_marks, idx_to_class.values(), rotation=45)\n",
    "    plt.yticks(tick_marks, idx_to_class.values())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(os.path.join(history_path, 'confusion_matrix.png'), bbox_inches=\"tight\")\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    indiv_acc = (confusion_matrix.diag()/confusion_matrix.sum(1)).numpy()\n",
    "\n",
    "    print(\"Test Accuracy : \" + str(avg_test_acc))\n",
    "\n",
    "    print(\"Test Accuracy Per Class :\")\n",
    "\n",
    "    for key in idx_to_class:\n",
    "        print(f\"{idx_to_class[key]} - {round(indiv_acc[key] * 100, 4)}%\")\n",
    "\n",
    "    with open(os.path.join(history_path, 'test.log'), 'a') as the_file:\n",
    "        the_file.write(\"Test Accuracy : \" + str(avg_test_acc) + \"\\n\\n\")\n",
    "        the_file.write(\"Test Accuracy Per Class : \\n\")\n",
    "        for key in idx_to_class:\n",
    "            the_file.write(f\"{idx_to_class[key]} - {round(indiv_acc[key] * 100, 4)}%\\n\")\n",
    "\n",
    "computeTestSetAccuracy(criterion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMTMERG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d3c456ffaa9cd28f83f00357dff394a6c471d38d2284dc7337cbaff2c725068"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
